import os
import json
import time
import base64
import io
from pathlib import Path
from typing import List, Dict, Tuple
from bs4 import BeautifulSoup
from PIL import Image

import gradio as gr
import torch
from transformers import AutoProcessor, Gemma3ForConditionalGeneration

# ==========================
# 1. å…¨å±€æ¨¡å‹åŠ è½½ï¼ˆåªè·‘ä¸€æ¬¡ï¼‰
# ==========================
MODEL_ID = r"D:\workspace\gemma-3-4b-it"  # <- æ”¹æˆä½ æœ¬åœ°çš„è·¯å¾„
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print("[INFO] æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™â€¦â€¦")
t0 = time.perf_counter()
model = Gemma3ForConditionalGeneration.from_pretrained(
    MODEL_ID,
    device_map=DEVICE,
    torch_dtype=torch.bfloat16,
).eval()
model.forward = torch._dynamo.disable(model.forward)

processor = AutoProcessor.from_pretrained(MODEL_ID)
t1 = time.perf_counter()
print(f"[INFO] æ¨¡å‹åŠ è½½å®Œæ¯•ï¼Œè€—æ—¶ {t1 - t0:.2f} s")

# ==========================
# 2. å¯¹è¯å‡½æ•°
# ==========================
def build_messages(history: List[List[str]],
         user_text: str,
         pil_imgs: List[Image.Image] | None) -> List[Dict]:
    """
    æŠŠ gradio history è½¬æˆ transformers æ‰€éœ€çš„ messages æ ¼å¼
    """
    messages = [{"role": "system",
                 "content": [{"type": "text", "text": "ä½ æ˜¯ä¸€ä¸ªä¸­æ–‡é—®ç­”å°åŠ©æ‰‹ã€‚"}]}]

    # é€æ¡è¿½åŠ å†å²è®°å½•
    for human, assistant in history:
        # ä»å¯èƒ½åŒ…å«HTMLçš„å†å²æ¶ˆæ¯ä¸­æå–çº¯æ–‡æœ¬
        soup = BeautifulSoup(human, 'html.parser')
        user_content_text = soup.get_text(separator=' ', strip=True)
        content = [{"type": "text", "text": user_content_text}]
        messages.append({"role": "user", "content": content})

        # assistant å›åˆ
        messages.append({"role": "assistant",
                         "content": [{"type": "text", "text": assistant}]})

    # æœ€æ–°ç”¨æˆ·è¾“å…¥
    final_content = []
    if pil_imgs:
        for img in pil_imgs:
            final_content.append({"type": "image", "image": img})
    final_content.append({"type": "text", "text": user_text})
    messages.append({"role": "user", "content": final_content})

    return messages


def chat_fn(history: List[List[str]],
            user_text: str,
            image_paths: List[str] | None) -> Tuple[List[List[str]], str]:
    """
    history -> [[user1, bot1], [user2, bot2], ...]
    user_text -> æœ¬æ¬¡è¾“å…¥æ–‡æœ¬
    image_paths  -> æœ¬æ¬¡ä¸Šä¼ å›¾ç‰‡è·¯å¾„åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

    è¿”å› (æ–°çš„ history, çŠ¶æ€ä¿¡æ¯)
    """
    if not user_text.strip() and not image_paths:
        return history, "è¯·è¾“å…¥æ–‡å­—å†…å®¹æˆ–ä¸Šä¼ å›¾ç‰‡ï¼"

    # å°†å›¾ç‰‡è·¯å¾„åˆ—è¡¨è½¬æ¢ä¸ºPIL.Imageå¯¹è±¡åˆ—è¡¨
    pil_imgs = [Image.open(path).convert("RGB") for path in image_paths] if image_paths else None

    messages = build_messages(history, user_text, pil_imgs)

    inputs = processor.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt"
    ).to(model.device)

    input_len = inputs["input_ids"].shape[-1]

    t2 = time.perf_counter()
    with torch.inference_mode():
        generation = model.generate(
            **inputs,
            max_new_tokens=512,
            do_sample=False,
            pad_token_id=processor.tokenizer.eos_token_id
        )
        generation = generation[0][input_len:]
    t3 = time.perf_counter()

    decoded = processor.decode(generation, skip_special_tokens=True)

    # å°†å›¾ç‰‡ç¼–ç ä¸º Base64 æ ¼å¼ï¼Œå¹¶æ„å»ºæ›´æ¸…æ™°çš„ HTML
    user_msg_html = user_text
    if pil_imgs:
        img_html_list = []
        for img in pil_imgs:
            # åˆ›å»ºç¼©ç•¥å›¾
            thumbnail_size = (100, 100)
            img.thumbnail(thumbnail_size)
            buffered = io.BytesIO()
            img.save(buffered, format="PNG")
            img_str = base64.b64encode(buffered.getvalue()).decode()
            
            # Gradio èŠå¤©æ¡†ä¼šè‡ªåŠ¨å¤„ç†ç‚¹å‡»å…¨å±ï¼Œæ‰€ä»¥è¿™é‡Œç›´æ¥ç”¨ img æ ‡ç­¾
            img_html = f'<img src="data:image/png;base64,{img_str}" style="max-width: 100px; max-height: 100px; margin: 5px; border-radius: 8px;">'
            img_html_list.append(img_html)
        
        img_html_str = "".join(img_html_list)
        user_msg_html = f'<div style="display: flex; flex-wrap: wrap;">{img_html_str}</div><p style="margin: 0; padding-top: 10px;">{user_text}</p>'


    # è¿½åŠ åˆ°å†å²
    history.append([user_msg_html, decoded])
    return history, f"æ¨ç†è€—æ—¶ {t3 - t2:.2f} s"

# ==========================
# 3. Gradio ç•Œé¢
# ==========================
css = """
.gradio-container {
    max-width: 1200px !important;
    margin: auto;
}
footer {
    visibility: hidden;
}
.gradio-row {
    flex-wrap: nowrap !important;
}
.gradio-column {
    min-width: 300px;
}
/* ä¼˜åŒ– Chatbot æ¶ˆæ¯æ°”æ³¡çš„æ ·å¼ */
.message-bubble {
    white-space: pre-wrap; /* ä¿æŒæ¢è¡Œ */
    word-wrap: break-word; /* å¼ºåˆ¶å•è¯æ¢è¡Œ */
    overflow-x: auto; /* é˜²æ­¢æ°´å¹³æº¢å‡º */
}
.message-bubble img {
    max-width: 100%;
}
"""

with gr.Blocks(title="Gemma-3-VLM å¯¹è¯åŠ©æ‰‹", css=css, theme="soft") as demo:
    gr.Markdown("# ğŸ’¬ Gemma-3-VLM å¯¹è¯åŠ©æ‰‹")
    gr.Markdown("ä¸Šä¼ å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰å¹¶è¾“å…¥æ–‡å­—ï¼Œå³å¯ä¸æ¨¡å‹å¯¹è¯ã€‚æ¨¡å‹åªåŠ è½½ä¸€æ¬¡ï¼Œåç»­å¯¹è¯ç§’çº§å“åº”ã€‚")

    # ä¸»ä½“å·¦å³åˆ†æ 
    with gr.Row():
        # å·¦ä¾§ï¼šèŠå¤©è®°å½•
        with gr.Column(scale=3):
            chatbot = gr.Chatbot(label="èŠå¤©è®°å½•", height="75vh", elem_classes="message-bubble")
        
        # å³ä¾§ï¼šè¾“å…¥åŒºåŸŸå’Œå›¾ç‰‡
        with gr.Column(scale=1):
            # å…³é”®ä¿®æ”¹ç‚¹ï¼šä½¿ç”¨ gr.Files ä»£æ›¿ gr.Image
            image_box = gr.Files(
                label="ä¸Šä¼ å›¾ç‰‡ï¼ˆæœ€å¤š15å¼ ï¼‰",
                file_count="multiple", # æŸäº›ç‰ˆæœ¬ä¹Ÿæ”¯æŒè¿™ç§å†™æ³•
                file_types=["image"],
            )
            text_box = gr.Textbox(
                placeholder="åœ¨æ­¤è¾“å…¥é—®é¢˜â€¦â€¦",
                lines=2,
                label="æ–‡å­—è¾“å…¥"
            )
            btn_send = gr.Button("å‘é€", variant="primary")
            status = gr.Textbox(label="çŠ¶æ€", interactive=False, container=False)
    
    # åº•éƒ¨æŒ‰é’®
    with gr.Row():
        btn_clear = gr.Button("æ¸…ç©ºå†å²")
        btn_export = gr.Button("å¯¼å‡ºèŠå¤©è®°å½•")

    # äº‹ä»¶ç»‘å®š
    def clear_fn():
        # ç¡®ä¿æ¸…ç©º image_box
        return [], None, ""

    def export_fn(history):
        path = Path("chat_history.json")
        with open(path, "w", encoding="utf-8") as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
        return str(path.absolute())

    btn_send.click(chat_fn,
                   inputs=[chatbot, text_box, image_box],
                   outputs=[chatbot, status])
    # è°ƒæ•´ clear_fn çš„è¾“å‡ºï¼Œä»¥æ¸…ç©º gr.Files
    btn_clear.click(clear_fn, outputs=[chatbot, image_box, status])
    btn_export.click(export_fn, inputs=chatbot, outputs=status)

    # å›è½¦å‘é€
    text_box.submit(chat_fn,
                    inputs=[chatbot, text_box, image_box],
                    outputs=[chatbot, status])

if __name__ == "__main__":
    demo.launch(share=True)