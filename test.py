"""
Gemma-3-VLM Chat Demo
è¿è¡Œæ–¹å¼ï¼š
    pip install gradio transformers accelerate torch pillow
    python app.py
"""

import os
import json
import time
from pathlib import Path
from typing import List, Dict, Tuple

import gradio as gr
import torch
from PIL import Image
from transformers import AutoProcessor, Gemma3ForConditionalGeneration

# ==========================
# 1. å…¨å±€æ¨¡å‹åŠ è½½ï¼ˆåªè·‘ä¸€æ¬¡ï¼‰
# ==========================
MODEL_ID = r"D:\workspace\gemma-3-4b-it"   # <- æ”¹æˆä½ æœ¬åœ°çš„è·¯å¾„
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print("[INFO] æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™â€¦â€¦")
t0 = time.perf_counter()
model = Gemma3ForConditionalGeneration.from_pretrained(
    MODEL_ID,
    device_map=DEVICE,
    torch_dtype=torch.bfloat16,
).eval()
model.forward = torch._dynamo.disable(model.forward)

processor = AutoProcessor.from_pretrained(MODEL_ID)
t1 = time.perf_counter()
print(f"[INFO] æ¨¡å‹åŠ è½½å®Œæ¯•ï¼Œè€—æ—¶ {t1 - t0:.2f} s")
# ==========================
# 2. å¯¹è¯å‡½æ•°
# ==========================
def build_messages(history: List[List[str]],
                   user_text: str,
                   pil_img: Image.Image | None) -> List[Dict]:
    """
    æŠŠ gradio history è½¬æˆ transformers æ‰€éœ€çš„ messages æ ¼å¼
    history ä¸­æ¯ä¸€é¡¹ç°åœ¨æ˜¯ [user_text, bot_text, pil_img | None]
    """
    messages = [{"role": "system",
                 "content": [{"type": "text", "text": "ä½ æ˜¯ä¸€ä¸ªä¸­æ–‡é—®ç­”å°åŠ©æ‰‹ã€‚"}]}]

    # é€æ¡è¿½åŠ å†å²è®°å½•
    for human, assistant, _ in history:               # ### æ”¹åŠ¨ï¼šè§£åŒ…æ—¶å¿½ç•¥å›¾ç‰‡
        content = [{"type": "text", "text": human}]
        messages.append({"role": "user", "content": content})
        messages.append({"role": "assistant",
                         "content": [{"type": "text", "text": assistant}]})

    # æœ€æ–°ç”¨æˆ·è¾“å…¥
    final_content = []
    if pil_img is not None:
        final_content.append({"type": "image", "image": pil_img})
    final_content.append({"type": "text", "text": user_text})
    messages.append({"role": "user", "content": final_content})

    return messages


def chat_fn(history: List[List[str]],
            user_text: str,
            pil_img: Image.Image | None) -> Tuple[List[List[str]], str]:
    """
    history  -> [[user1, bot1, img1], [user2, bot2, img2], ...]
    user_text -> æœ¬æ¬¡è¾“å…¥æ–‡æœ¬
    pil_img   -> æœ¬æ¬¡ä¸Šä¼ å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰

    è¿”å› (æ–°çš„ history, çŠ¶æ€ä¿¡æ¯)
    """
    if not user_text.strip():
        return history, "è¯·è¾“å…¥æ–‡å­—å†…å®¹ï¼"

    messages = build_messages(history, user_text, pil_img)

    inputs = processor.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt"
    ).to(model.device)

    input_len = inputs["input_ids"].shape[-1]

    t2 = time.perf_counter()
    with torch.inference_mode():
        generation = model.generate(
            **inputs,
            max_new_tokens=512,
            do_sample=False,
            pad_token_id=processor.tokenizer.eos_token_id
        )
        generation = generation[0][input_len:]
    t3 = time.perf_counter()

    decoded = processor.decode(generation, skip_special_tokens=True)

    # ### æ”¹åŠ¨ï¼šæŠŠæœ¬æ¬¡å›¾ç‰‡ä¸€èµ·å­˜è¿›å†å²
    history.append([user_text, decoded, pil_img])
    return history, f"æ¨ç†è€—æ—¶ {t3 - t2:.2f} s"

# ==========================
# 3. Gradio ç•Œé¢
# ==========================
css = """
.gradio-container {max-width: 900px !important}
footer {visibility: hidden}
"""

with gr.Blocks(title="Gemma-3-VLM å¯¹è¯åŠ©æ‰‹", css=css, theme="soft") as demo:
    gr.Markdown("# ğŸ’¬ Gemma-3-VLM å¯¹è¯åŠ©æ‰‹")
    gr.Markdown("ä¸Šä¼ å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰å¹¶è¾“å…¥æ–‡å­—ï¼Œå³å¯ä¸æ¨¡å‹å¯¹è¯ã€‚æ¨¡å‹åªåŠ è½½ä¸€æ¬¡ï¼Œåç»­å¯¹è¯ç§’çº§å“åº”ã€‚")

    with gr.Row():
        with gr.Column(scale=3):
            # ### æ”¹åŠ¨ï¼šæŠŠ Chatbot çš„ elem å±æ€§æ‰“å¼€ï¼Œè®©å®ƒæ”¯æŒå›¾ç‰‡
            chatbot = gr.Chatbot(label="èŠå¤©è®°å½•", height=500, type="messages")
        with gr.Column(scale=1):
            image_box = gr.Image(type="pil", label="ä¸Šä¼ å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰")
            text_box = gr.Textbox(
                placeholder="åœ¨æ­¤è¾“å…¥é—®é¢˜â€¦â€¦",
                lines=2,
                label="æ–‡å­—è¾“å…¥"
            )
            btn_send = gr.Button("å‘é€", variant="primary")
            status = gr.Textbox(label="çŠ¶æ€", interactive=False)

    # å¿«æ·æŒ‰é’®
    with gr.Row():
        btn_clear = gr.Button("æ¸…ç©ºå†å²")
        btn_export = gr.Button("å¯¼å‡ºèŠå¤©è®°å½•")

    # äº‹ä»¶ç»‘å®š
    def clear_fn():
        return [], ""  # ### æ”¹åŠ¨ï¼šè¿”å›ç©ºåˆ—è¡¨ï¼Œæ ¼å¼ä¸ history ä¸€è‡´

    def export_fn(history):
        # å¯¼å‡ºæ—¶å»æ‰å›¾ç‰‡ï¼Œé¿å… JSON æ— æ³•åºåˆ—åŒ– PIL å¯¹è±¡
        export_data = [[h[0], h[1]] for h in history]
        path = Path("chat_history.json")
        with open(path, "w", encoding="utf-8") as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        return str(path.absolute())

    btn_send.click(chat_fn,
                   inputs=[chatbot, text_box, image_box],
                   outputs=[chatbot, status])
    btn_clear.click(clear_fn, outputs=[chatbot, status])
    btn_export.click(export_fn, inputs=chatbot, outputs=status)

    # å›è½¦å‘é€
    text_box.submit(chat_fn,
                    inputs=[chatbot, text_box, image_box],
                    outputs=[chatbot, status])

if __name__ == "__main__":
    demo.launch(share=False)